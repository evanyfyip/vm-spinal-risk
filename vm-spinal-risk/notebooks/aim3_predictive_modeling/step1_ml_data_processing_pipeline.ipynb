{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7e0912",
   "metadata": {},
   "source": [
    "### Step 1 ML Data Processing Pipeline\n",
    "**Author:** Evan Yip <br>\n",
    "**Purpose:** The primary purpose of this notebook is to take the raw csv file, extract and transform the data features and perform the necessary scalings and one hot encodings to prepare the data for either of the `step2_ml_pipeline*.ipynb` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e37ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "##########################################################################################\n",
    "#######       Set the current working directory to the root of your project     ##########\n",
    "os.chdir(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n",
    "##########################################################################################\n",
    "\n",
    "# Local imports\n",
    "from utilities import data_processing as dp_utils\n",
    "from utilities.drop_unbalanced_features import DropUnbalancedFeatures\n",
    "\n",
    "# Setting pandas display options for maximum column display\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c457",
   "metadata": {},
   "source": [
    "### Extract additional data features from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b455dc4",
   "metadata": {},
   "source": [
    "Using the methods that are in `../utilities/data_processing.py` we will extract and transform the data to gain additional columns such as aggregated DOSPERT scores, height in meters, weight in kg, BMI, ADI and our calculated spinal risk score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f59d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the data\n",
    "# df_features = pd.read_csv('./data/data_raw/RiskFinal_DATA_2024-02-05_0017_combined.csv')\n",
    "# # Transforming features\n",
    "# # second argument may change based on how the data looks, respecify the index where the dospert questions start.\n",
    "# df_features = dp_utils.get_dospert_scores(df_features, 60)\n",
    "# df_features['height_m'] = df_features.height.apply(lambda h: dp_utils.get_height_value(value=h, unit='metric'))/100\n",
    "# df_features['weight_kg'] = df_features.weight.apply(lambda h: dp_utils.get_weight_value(value=h, unit='metric'))\n",
    "# df_features['bmi'] = df_features[['height_m', 'weight_kg']].apply(lambda row: dp_utils.compute_bmi(row.height_m, row.weight_kg), axis=1)\n",
    "# df_features = dp_utils.get_age_ranges(df_features, age_column='age')\n",
    "# df_features = dp_utils.get_location_information(df_features)\n",
    "# df_features = dp_utils.get_adi_score(df_features)\n",
    "# df_features = dp_utils.get_spinal_risk_score(df_features)\n",
    "\n",
    "# # Dropping records based on low variance results\n",
    "# df_features = dp_utils.manual_drop_records(df_features)\n",
    "\n",
    "# # Write to csv\n",
    "# df_features.to_csv('./data/data_processed/all_risk_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b179cf",
   "metadata": {},
   "source": [
    "### Loading the data features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab101675-368d-4ddf-8012-76974e735caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799, 86)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/data_processed/all_risk_processed.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c16b7c-815c-48ee-962f-97206174812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the risk scenario questions for later\n",
    "risk_df = data.filter(regex='exer_|work_').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38510443-2979-449a-8bfd-516089dbffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the irrelevant columns for our ML models\n",
    "data_final = data.drop(['odi_1', 'odi_2', 'odi_3',\n",
    "       'odi_4', 'odi_5', 'odi_6', 'odi_7', 'odi_8', 'odi_9', 'odi_10',\n",
    "       'exer_50improv_1drop', 'exer_50improv_10drop', 'exer_50improv_50drop',\n",
    "       'exer_50improv_90drop', 'att_check_1', 'exer_90improv_1drop',\n",
    "       'exer_90improv_10drop', 'exer_90improv_50drop', 'exer_90improv_90drop',\n",
    "       'exer_50pain_1death', 'exer_50pain_10death', 'exer_50pain_50death',\n",
    "       'exer_90pain_1death', 'exer_90pain_10death', 'exer_90pain_50death',\n",
    "       'work_50improv_1drop', 'work_50improv_10drop', 'work_50improv_50drop',\n",
    "       'work_50improv_90drop', 'work_90improv_1drop', 'work_90improv_10drop',\n",
    "       'work_90improv_50drop', 'work_50improv_1para', 'work_50improv_10para',\n",
    "       'work_50improv_50para', 'work_50improv_90para', 'work_90improv_1para',\n",
    "       'work_90improv_10para', 'att_check2', 'work_90improv_50para',\n",
    "       'work_50improv_1death', 'work_50improv_10death',\n",
    "       'work_50improv_50death', 'work_90improv_1death',\n",
    "       'work_90improv_10death', 'work_90improv_50death', 'att_pass',\n",
    "       'risk_1_complete','height', 'weight', 'record_id', 'risk_1_timestamp', \n",
    "       'zipcode','age_range', 'postal_code','state_code','city',\n",
    "       'province', 'province_code','latitude', 'longitude', 'FIPS', 'fips', 'GISJOIN', 'state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f99f18-0176-485d-a9b7-85c3213195e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ADI into an integer value\n",
    "data_final['ADI_NATRANK'] = pd.to_numeric(data_final['ADI_NATRANK'], errors='coerce').astype(float).astype('Int64')\n",
    "data_final['ADI_STATERNK'] = pd.to_numeric(data_final['ADI_STATERNK'], errors='coerce').astype(float).astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc6142",
   "metadata": {},
   "source": [
    "### ML Processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f3e40",
   "metadata": {},
   "source": [
    "The next step is to one hot encode the nonordinal categorical variables (`ohe_cols`) and impute and scale the ordinal categorical variables (`cat_cols`) and the numerical columns (`num_cols`). <br> \n",
    "\n",
    "Additionally, I will be applying my own DropUnbalancedFeatures transformation to remove categorical columns that are highly unbalanced (e.g. 90% religious and 10% nonreligious)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f397b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the columns fo the preprocessing pipeline\n",
    "ohe_cols = [\"religion\", \"ethnicity\"]\n",
    "cat_cols = [\"sex\", \"income\", \"education\", \"prior_surg\", \"spin_surg\", \"succ_surg\"]\n",
    "num_cols = [\"age\", \"odi_final\", \"bmi\", \"dospert_ethical\", \"dospert_financial\", \"dospert_health/safety\", \"dospert_recreational\", \"dospert_social\", \"height_m\", \"weight_kg\", \"ADI_NATRANK\", \"ADI_STATERNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df03623b-dd54-4806-9a9a-79dd49443225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing pipeline\n",
    "ohe_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')),\n",
    "    ('selector', DropUnbalancedFeatures(threshold=0.8, verbose=False))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('selector', DropUnbalancedFeatures(threshold=0.8, verbose=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', IterativeImputer(random_state=52)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ohe', ohe_pipe, ohe_cols),\n",
    "        ('cat', cat_pipe, cat_cols),\n",
    "        ('num', num_pipe, num_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bbe1ced-b65d-4e87-b4a5-499ee599c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing pipeline\n",
    "processed_final = preprocessor.fit_transform(data_final)\n",
    "transformed_columns = preprocessor.get_feature_names_out(input_features=data_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7da53",
   "metadata": {},
   "source": [
    "### Saving the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaec7aa",
   "metadata": {},
   "source": [
    "For this step we save two separate data files. We take the `processed_final` data from the preprocessor and save it as `./data/data_processed/ml_data_processed_final.csv`. This will be used for the `step2_ml_pipeline_risk_model.ipynb`. However we need an additional data file that contains the risk scenario questions. This will be saved as `./data/data_processed/ml_data_w_risk_questions_processed_final.csv`. This file will be used for `step2_ml_pipeline_choice_model.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a76e642-bb22-4140-a9cc-14c6daf360de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving file for step2_ml_pipeline_risk_model.ipynb\n",
    "processed_final_df = pd.DataFrame(processed_final, columns=transformed_columns)\n",
    "processed_final_df['spinal_risk_score'] = data_final['spinal_risk_score'].values\n",
    "processed_final_df.to_csv('./data/data_processed/ml_data_processed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10de87b-8a74-411d-ae82-4ec4567a592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving file for step2_ml_pipeline_choice_model.ipynb\n",
    "processed_w_risk_final_df = pd.concat([processed_final_df, risk_df], axis=1)\n",
    "processed_w_risk_final_df.to_csv('./data/data_processed/ml_data_w_risk_questions_processed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba7723",
   "metadata": {},
   "source": [
    "### Saving the fitted preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ab7b4",
   "metadata": {},
   "source": [
    "Finally, we will save the preprocessing pipeline to a pickle object so that when we want to transform new data and make predictions using our ML models, we can call the fitted preprocessing pipeline and apply the same transformations on that new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f35aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving preprocessing pipeline to pickle object\n",
    "import pickle\n",
    "with open('./data/ml_models/general_model_preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
